{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ1_H40DBRTc",
        "outputId": "419613ac-6005-4c8b-9531-30ffb040cd2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n9tSyIsBfdx",
        "outputId": "534da3f8-897b-40e0-bc94-cbe515fabb32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
        "\n",
        "# Get SparkContext\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "iKv_9sjehrZh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "9ZtAMvF0E-dM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB8T8uABBk6K",
        "outputId": "578a5abf-2331-42f0-ae8c-bdb795b4c544"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/spark-1/README.md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhDhekb2BzUS",
        "outputId": "4df3eef5-67ad-4449-8b3a-adbaa4d6b45e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/spark-1/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, col"
      ],
      "metadata": {
        "id": "5qYJZMSgCkhB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/spark-1/README.md\"\n",
        "rdd = sc.textFile(file_path)"
      ],
      "metadata": {
        "id": "H8p9tWUKDUAN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read file into RDD\n",
        "text_rdd = spark.sparkContext.textFile(file_path)"
      ],
      "metadata": {
        "id": "NUmQukcQC9m6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start timer to measure execution time\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "pDUwAp61Cwoz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform word count\n",
        "word_counts = (\n",
        "    rdd.flatMap(lambda line: line.split())  # Split lines into words\n",
        "       .map(lambda word: (word, 1))         # Assign count 1 to each word\n",
        "       .reduceByKey(lambda a, b: a + b)     # Aggregate counts\n",
        ")\n"
      ],
      "metadata": {
        "id": "mKjtWjBCFPfN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect results\n",
        "word_counts_collected = word_counts.collect()\n"
      ],
      "metadata": {
        "id": "t1kvBHJSFjDR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time"
      ],
      "metadata": {
        "id": "bvAacVeFFqU4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to dictionary for analysis\n",
        "word_counts_dict = dict(word_counts_collected)"
      ],
      "metadata": {
        "id": "8et6B_hwFumT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Hadoop count\n",
        "hadoop_count = word_counts_dict.get(\"Hadoop\", 0)"
      ],
      "metadata": {
        "id": "-bc7pQrRFzum"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the most common  words\n",
        "most_common_word = max(word_counts_dict, key=word_counts_dict.get)\n",
        "most_common_count = word_counts_dict[most_common_word]"
      ],
      "metadata": {
        "id": "BE1vopwUGAez"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the  least common words\n",
        "least_common_words = [word for word, count in word_counts_dict.items() if count == min(word_counts_dict.values())]\n",
        "least_common_count = min(word_counts_dict.values())"
      ],
      "metadata": {
        "id": "cj3p_3rHGEuo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "print(f'Hadoop Count: {hadoop_count}')\n",
        "print(f'Most Common Word: \"{most_common_word}\" appears {most_common_count} times')\n",
        "print(f'Least Common Word(s): {least_common_words}')\n",
        "print(f'Least Common Word Count: {least_common_count}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drSpQ3qFGUjW",
        "outputId": "e4f65636-0f39-4cf3-dcf2-87e6de20b7c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 11.99 seconds\n",
            "Hadoop Count: 3\n",
            "Most Common Word: \"the\" appears 23 times\n",
            "Least Common Word(s): ['#', 'analytics', 'Scala,', 'optimized', 'analysis.', 'higher-level', 'workloads,', 'MLlib', 'machine', 'learning,', 'GraphX', 'graph', 'processing,', '<https://spark.apache.org/>', '[![AppVeyor', '[![PySpark', '[![PyPI', 'Documentation', 'find', 'latest', '[project', 'web', 'README', 'file', 'basic', 'setup', 'Building', 'built', 'run:', './build/mvn', '-DskipTests', 'clean', 'this', 'available', 'from', 'site,', '[\"Building', 'Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 'IDE,', '[\"Useful', 'Developer', 'easiest', 'start', '```scala', 'scala>', 'comes', 'sample', 'directory.', 'them,', '`./bin/run-example', 'will', 'MASTER', 'environment', 'running', 'submit', 'cluster.', 'mesos://', 'spark://', 'YARN,', '\"local\"', 'thread,', '\"local[N]\"', 'threads.', 'name', 'instance:', 'MASTER=spark://host:7077', 'usage', 'help', 'no', 'params', 'are', 'Running', 'Tests', 'first', 'requires', 'Spark](#building-spark).', 'built,', './dev/run-tests', 'module,', 'There', 'integration', 'test,', 'A', 'About', 'uses', 'core', 'library', 'other', 'storage', 'systems.', 'Because', 'protocols', 'have', 'changed', 'against', 'version', 'Enabling', 'YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 'distribution', 'Configuration', '[Configuration', 'online', 'configure', 'Spark.', 'review', 'guide](https://spark.apache.org/contributing.html)', 'information', 'started', 'project.', 'Apache', 'unified', 'large-scale', 'provides', 'high-level', 'APIs', 'Java,', 'R,', 'computation', 'graphs', 'rich', 'tools', 'DataFrames,', 'API', 'Structured', 'Streaming', 'stream', '[![GitHub', 'Actions', 'Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)', 'Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)', 'Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)', 'Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)', 'Online', 'documentation,', 'programming', 'guide,', 'page](https://spark.apache.org/documentation.html).', 'only', 'contains', 'instructions.', '[Apache', 'Maven](https://maven.apache.org/).', 'its', 'programs,', 'package', '(You', 'not', 'need', 'downloaded', 'pre-built', 'package.)', 'More', 'project', 'development', 'tips,', 'info', 'developing', 'Tools\"](https://spark.apache.org/developer-tools.html).', 'The', 'way', 'through', './bin/spark-shell', 'Try', 'Alternatively,', 'prefer', './bin/pyspark', 'And', '```python', '>>>', 'Example', 'Programs', 'several', '<class>', '[params]`.', 'example:', 'Pi', 'locally.', 'variable', 'when', 'URL,', '\"yarn\"', 'N', 'abbreviated', 'package.', 'Many', 'print', 'given.', 'Testing', '[building', 'Once', 'using:', '[run', 'individual', 'tests](https://spark.apache.org/developer-tools.html#individual-tests).', 'Kubernetes', 'resource-managers/kubernetes/integration-tests/README.md', 'Note', 'Versions', 'talk', 'HDFS', 'Hadoop-supported', 'different', 'versions', 'must', 'same', 'your', 'cluster', 'runs.', '[\"Specifying', 'Version', 'Thriftserver', 'distributions.', 'Guide](https://spark.apache.org/docs/latest/configuration.html)', 'overview', 'Contributing', '[Contribution', 'get', 'contributing']\n",
            "Least Common Word Count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show job execution details\n",
        "print(\"\\nCheck the Spark UI at: http://<your_colab_ip>:4040/jobs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IphB8qdzGs9Y",
        "outputId": "7140d3d6-7fe3-444a-d6ee-c8a23c336354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Check the Spark UI at: http://<your_colab_ip>:4040/jobs\n"
          ]
        }
      ]
    }
  ]
}